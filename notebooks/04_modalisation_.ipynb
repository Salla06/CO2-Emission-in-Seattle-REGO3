{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f516adf",
   "metadata": {},
   "source": [
    "# üè¢ PR√âDICTION DES √âMISSIONS DE GES - B√ÇTIMENTS DE SEATTLE\n",
    "## Notebook 04 : Mod√©lisation Professionnelle (Version Modulaire)\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Informations du Projet\n",
    "\n",
    "**Projet** : Seattle Energy Benchmarking - Pr√©diction des √©missions de CO‚ÇÇ  \n",
    "**Dataset** : 1,666 b√¢timents non-r√©sidentiels (2015-2016)  \n",
    "**Objectif** : Comparer deux approches pr√©dictives  \n",
    "**Date** : Janvier 2026  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Strat√©gie de Mod√©lisation\n",
    "\n",
    "Ce notebook impl√©mente une **architecture modulaire professionnelle** avec :\n",
    "\n",
    "‚úÖ **S√©paration des responsabilit√©s** : Code r√©utilisable dans `src/`  \n",
    "‚úÖ **Fonctions test√©es** : Modules document√©s et maintenables  \n",
    "‚úÖ **Notebook √©pur√©** : Focus sur l'analyse, pas l'impl√©mentation  \n",
    "‚úÖ **Bonnes pratiques** : Structure de projet professionnelle  \n",
    "\n",
    "### üìä Les 2 Mod√®les\n",
    "\n",
    "1. **Mod√®le 1 (Pr√©dictif Pur)** : Variables disponibles au permis de construction\n",
    "2. **Mod√®le 2 (Performance Optimale)** : Avec ENERGY STAR Score\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685bc689",
   "metadata": {},
   "source": [
    "# Section 0 : Configuration et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PROJECT_ROOT : c:\\Users\\LENOVO\\CO2-Emission-in-Seattle-REGO3\n",
      "‚úì SRC_PATH : c:\\Users\\LENOVO\\CO2-Emission-in-Seattle-REGO3\\src\n",
      "‚úì Chemin src ajout√© au PATH\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì Chemin src ajout√© au PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# IMPORTS MODULES PERSONNALIS√âS\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     get_model_param_grid,\n\u001b[0;32m     43\u001b[0m     optimize_model,\n\u001b[0;32m     44\u001b[0m     train_multiple_models,\n\u001b[0;32m     45\u001b[0m     save_model,\n\u001b[0;32m     46\u001b[0m     load_model,\n\u001b[0;32m     47\u001b[0m     create_stacking_model,\n\u001b[0;32m     48\u001b[0m     get_feature_importance,\n\u001b[0;32m     49\u001b[0m     compare_models\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m     evaluate_model,\n\u001b[0;32m     54\u001b[0m     cv_evaluate_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     compare_model_performance\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     63\u001b[0m     plot_predictions,\n\u001b[0;32m     64\u001b[0m     plot_residuals_distribution,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m     plot_comparison_two_models\n\u001b[0;32m     69\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS STANDARDS\n",
    "# ============================================================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# AJOUTER src AU PATH - APPROCHE ROBUSTE\n",
    "# ============================================================================\n",
    "\n",
    "# Option 1 : Depuis le r√©pertoire courant\n",
    "try:\n",
    "    # Essayer d'abord avec Path courant\n",
    "    current_path = Path.cwd()\n",
    "    print(f\"R√©pertoire courant : {current_path}\")\n",
    "    \n",
    "    # Si on est dans notebooks, remonter au parent\n",
    "    if current_path.name == 'notebooks':\n",
    "        PROJECT_ROOT = current_path.parent\n",
    "    # Si on est √† la racine\n",
    "    elif (current_path / 'notebooks').exists():\n",
    "        PROJECT_ROOT = current_path\n",
    "    # Sinon, chercher le dossier notebooks\n",
    "    else:\n",
    "        PROJECT_ROOT = current_path.parent\n",
    "    \n",
    "    SRC_PATH = PROJECT_ROOT / 'src'\n",
    "    \n",
    "    print(f\"PROJECT_ROOT : {PROJECT_ROOT}\")\n",
    "    print(f\"SRC_PATH : {SRC_PATH}\")\n",
    "    print(f\"src existe : {SRC_PATH.exists()}\")\n",
    "    \n",
    "    # Ajouter au path\n",
    "    if str(SRC_PATH) not in sys.path:\n",
    "        sys.path.insert(0, str(SRC_PATH))\n",
    "    \n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    \n",
    "    print(\"‚úì Chemins ajout√©s au PATH\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors du setup des chemins : {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS MODULES PERSONNALIS√âS\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    # Importer directement les modules depuis leur fichier\n",
    "    import importlib.util\n",
    "    \n",
    "    # Load modeling_utils\n",
    "    spec_modeling = importlib.util.spec_from_file_location(\"modeling_utils\", SRC_PATH / \"modeling_utils.py\")\n",
    "    modeling_utils = importlib.util.module_from_spec(spec_modeling)\n",
    "    spec_modeling.loader.exec_module(modeling_utils)\n",
    "    \n",
    "    # Load evaluation_utils\n",
    "    spec_eval = importlib.util.spec_from_file_location(\"evaluation_utils\", SRC_PATH / \"evaluation_utils.py\")\n",
    "    evaluation_utils = importlib.util.module_from_spec(spec_eval)\n",
    "    spec_eval.loader.exec_module(evaluation_utils)\n",
    "    \n",
    "    # Load visualization_utils\n",
    "    spec_viz = importlib.util.spec_from_file_location(\"visualization_utils\", SRC_PATH / \"visualization_utils.py\")\n",
    "    visualization_utils = importlib.util.module_from_spec(spec_viz)\n",
    "    spec_viz.loader.exec_module(visualization_utils)\n",
    "    \n",
    "    # Extraire les fonctions\n",
    "    get_model_param_grid = modeling_utils.get_model_param_grid\n",
    "    optimize_model = modeling_utils.optimize_model\n",
    "    train_multiple_models = modeling_utils.train_multiple_models\n",
    "    save_model = modeling_utils.save_model\n",
    "    load_model = modeling_utils.load_model\n",
    "    create_stacking_model = modeling_utils.create_stacking_model\n",
    "    get_feature_importance = modeling_utils.get_feature_importance\n",
    "    compare_models = modeling_utils.compare_models\n",
    "    \n",
    "    evaluate_model = evaluation_utils.evaluate_model\n",
    "    cv_evaluate_model = evaluation_utils.cv_evaluate_model\n",
    "    compute_residuals_stats = evaluation_utils.compute_residuals_stats\n",
    "    test_homoscedasticity = evaluation_utils.test_homoscedasticity\n",
    "    evaluate_prediction_quality = evaluation_utils.evaluate_prediction_quality\n",
    "    calculate_metrics_summary = evaluation_utils.calculate_metrics_summary\n",
    "    compare_model_performance = evaluation_utils.compare_model_performance\n",
    "    \n",
    "    plot_predictions = visualization_utils.plot_predictions\n",
    "    plot_residuals_distribution = visualization_utils.plot_residuals_distribution\n",
    "    plot_feature_importance = visualization_utils.plot_feature_importance\n",
    "    plot_model_comparison = visualization_utils.plot_model_comparison\n",
    "    plot_learning_curves = visualization_utils.plot_learning_curves\n",
    "    plot_comparison_two_models = visualization_utils.plot_comparison_two_models\n",
    "    \n",
    "    print(\"‚úì Modules personnalis√©s import√©s avec succ√®s !\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erreur d'import : {e}\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# SCIKIT-LEARN\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# XGBoost (optionnel)\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"‚úì XGBoost disponible\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚úó XGBoost non disponible\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPORTS TERMIN√âS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df84d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION GLOBALE\n",
    "# ============================================================================\n",
    "\n",
    "# Seeds\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configuration pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Configuration matplotlib/seaborn\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CHEMINS\n",
    "# ============================================================================\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "INTERIM_DIR = DATA_DIR / 'interim_data'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "FIGURES_DIR = RESULTS_DIR / 'figures'\n",
    "\n",
    "# Cr√©er dossiers\n",
    "for directory in [MODELS_DIR, RESULTS_DIR, FIGURES_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration termin√©e\")\n",
    "print(f\"\\nChemins configur√©s :\")\n",
    "print(f\"  Donn√©es    : {INTERIM_DIR}\")\n",
    "print(f\"  Mod√®les    : {MODELS_DIR}\")\n",
    "print(f\"  R√©sultats  : {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b95f16",
   "metadata": {},
   "source": [
    "# Section 1 : Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af215381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHARGEMENT DES DONN√âES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Charger\n",
    "train_df = pd.read_csv(INTERIM_DIR / 'train_with_features.csv')\n",
    "test_df = pd.read_csv(INTERIM_DIR / 'test_with_features.csv')\n",
    "\n",
    "print(f\"\\n‚úì Train : {train_df.shape}\")\n",
    "print(f\"‚úì Test  : {test_df.shape}\")\n",
    "\n",
    "# Aper√ßu\n",
    "print(\"\\nAper√ßu des donn√©es :\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064e2c9",
   "metadata": {},
   "source": [
    "# Section 2 : D√©finition des Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a47f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VARIABLES\n",
    "# ============================================================================\n",
    "\n",
    "# Target\n",
    "TARGET = 'TotalGHGEmissions_log'\n",
    "\n",
    "# V√©rifier et cr√©er si n√©cessaire\n",
    "if TARGET not in train_df.columns:\n",
    "    print(f\"‚ö† {TARGET} non trouv√©e. Cr√©ation...\")\n",
    "    train_df[TARGET] = np.log1p(train_df['TotalGHGEmissions'])\n",
    "    test_df[TARGET] = np.log1p(test_df['TotalGHGEmissions'])\n",
    "    print(f\"‚úì {TARGET} cr√©√©e\")\n",
    "\n",
    "# MOD√àLE 1 : Variables autoris√©es\n",
    "variables_autorisees = [\n",
    "    'BuildingType', 'PrimaryPropertyType', 'City', 'State', 'ZipCode',\n",
    "    'CouncilDistrictCode', 'Neighborhood', 'Latitude', 'Longitude',\n",
    "    'YearBuilt', 'NumberofBuildings', 'NumberofFloors',\n",
    "    'PropertyGFATotal', 'PropertyGFAParking', 'PropertyGFABuilding(s)',\n",
    "    'ListOfAllPropertyUseTypes', 'LargestPropertyUseType',\n",
    "    'LargestPropertyUseTypeGFA', 'SecondLargestPropertyUseType',\n",
    "    'SecondLargestPropertyUseTypeGFA', 'ThirdLargestPropertyUseType',\n",
    "    'ThirdLargestPropertyUseTypeGFA'\n",
    "]\n",
    "\n",
    "# Filtrer variables existantes\n",
    "variables_autorisees_existantes = [\n",
    "    v for v in variables_autorisees if v in train_df.columns\n",
    "]\n",
    "\n",
    "# Si peu de variables trouv√©es, utiliser toutes les features\n",
    "if len(variables_autorisees_existantes) < 5:\n",
    "    print(\"\\n‚ö† Variables pr√©d√©finies non trouv√©es. Utilisation de toutes les features.\")\n",
    "    exclude_cols = ['OSEBuildingID', 'DataYear', 'TotalGHGEmissions', TARGET, \n",
    "                    'ENERGYSTARScore', 'PropertyName', 'Address']\n",
    "    variables_autorisees_existantes = [\n",
    "        col for col in train_df.columns if col not in exclude_cols\n",
    "    ]\n",
    "\n",
    "# MOD√àLE 2 : Avec ENERGY STAR\n",
    "variables_exp_tot = variables_autorisees_existantes.copy()\n",
    "if 'ENERGYSTARScore' in train_df.columns:\n",
    "    variables_exp_tot.append('ENERGYSTARScore')\n",
    "\n",
    "print(f\"\\nüìä MOD√àLE 1 : {len(variables_autorisees_existantes)} features\")\n",
    "print(f\"üìä MOD√àLE 2 : {len(variables_exp_tot)} features\")\n",
    "print(f\"üéØ Target : {TARGET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d719c2e2",
   "metadata": {},
   "source": [
    "# Section 3 : Pr√©paration des Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PR√âPARATION MOD√àLE 1\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PR√âPARATION MOD√àLE 1 - SANS ENERGY STAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extraire target\n",
    "y_train = train_df[TARGET]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "# Features Mod√®le 1\n",
    "X_train_m1 = train_df[variables_autorisees_existantes].copy()\n",
    "X_test_m1 = test_df[variables_autorisees_existantes].copy()\n",
    "\n",
    "# G√©rer variables cat√©gorielles\n",
    "categorical_cols = X_train_m1.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"\\n‚ö† {len(categorical_cols)} variables cat√©gorielles d√©tect√©es\")\n",
    "    print(\"   ‚Üí Encodage One-Hot\")\n",
    "    X_train_m1 = pd.get_dummies(X_train_m1, columns=categorical_cols, drop_first=True)\n",
    "    X_test_m1 = pd.get_dummies(X_test_m1, columns=categorical_cols, drop_first=True)\n",
    "    X_train_m1, X_test_m1 = X_train_m1.align(X_test_m1, join='left', axis=1, fill_value=0)\n",
    "    print(f\"   ‚úì {X_train_m1.shape[1]} features apr√®s encodage\")\n",
    "\n",
    "# Scaling\n",
    "print(\"\\nüîÑ Scaling...\")\n",
    "scaler_m1 = StandardScaler()\n",
    "X_train_m1_scaled = scaler_m1.fit_transform(X_train_m1)\n",
    "X_test_m1_scaled = scaler_m1.transform(X_test_m1)\n",
    "\n",
    "# Convertir en DataFrame\n",
    "X_train_m1_scaled = pd.DataFrame(X_train_m1_scaled, columns=X_train_m1.columns, index=X_train_m1.index)\n",
    "X_test_m1_scaled = pd.DataFrame(X_test_m1_scaled, columns=X_test_m1.columns, index=X_test_m1.index)\n",
    "\n",
    "print(f\"\\n‚úì Mod√®le 1 pr√™t :\")\n",
    "print(f\"  X_train : {X_train_m1_scaled.shape}\")\n",
    "print(f\"  X_test  : {X_test_m1_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdbd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PR√âPARATION MOD√àLE 2\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PR√âPARATION MOD√àLE 2 - AVEC ENERGY STAR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Features Mod√®le 2\n",
    "X_train_m2 = train_df[variables_exp_tot].copy()\n",
    "X_test_m2 = test_df[variables_exp_tot].copy()\n",
    "\n",
    "# Imputer ENERGY STAR si valeurs manquantes\n",
    "if 'ENERGYSTARScore' in X_train_m2.columns:\n",
    "    missing = X_train_m2['ENERGYSTARScore'].isnull().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"\\n‚ö† {missing} valeurs manquantes ENERGYSTARScore\")\n",
    "        median_val = X_train_m2['ENERGYSTARScore'].median()\n",
    "        X_train_m2['ENERGYSTARScore'].fillna(median_val, inplace=True)\n",
    "        X_test_m2['ENERGYSTARScore'].fillna(median_val, inplace=True)\n",
    "        print(f\"   ‚úì Imputation avec m√©diane = {median_val:.2f}\")\n",
    "\n",
    "# G√©rer variables cat√©gorielles\n",
    "categorical_cols_m2 = X_train_m2.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if len(categorical_cols_m2) > 0:\n",
    "    print(f\"\\n‚ö† {len(categorical_cols_m2)} variables cat√©gorielles\")\n",
    "    X_train_m2 = pd.get_dummies(X_train_m2, columns=categorical_cols_m2, drop_first=True)\n",
    "    X_test_m2 = pd.get_dummies(X_test_m2, columns=categorical_cols_m2, drop_first=True)\n",
    "    X_train_m2, X_test_m2 = X_train_m2.align(X_test_m2, join='left', axis=1, fill_value=0)\n",
    "    print(f\"   ‚úì {X_train_m2.shape[1]} features apr√®s encodage\")\n",
    "\n",
    "# Scaling\n",
    "print(\"\\nüîÑ Scaling...\")\n",
    "scaler_m2 = StandardScaler()\n",
    "X_train_m2_scaled = scaler_m2.fit_transform(X_train_m2)\n",
    "X_test_m2_scaled = scaler_m2.transform(X_test_m2)\n",
    "\n",
    "X_train_m2_scaled = pd.DataFrame(X_train_m2_scaled, columns=X_train_m2.columns, index=X_train_m2.index)\n",
    "X_test_m2_scaled = pd.DataFrame(X_test_m2_scaled, columns=X_test_m2.columns, index=X_test_m2.index)\n",
    "\n",
    "print(f\"\\n‚úì Mod√®le 2 pr√™t :\")\n",
    "print(f\"  X_train : {X_train_m2_scaled.shape}\")\n",
    "print(f\"  X_test  : {X_test_m2_scaled.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì PR√âPARATION TERMIN√âE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab678dc",
   "metadata": {},
   "source": [
    "# Section 4 : MOD√àLE 1 - Baseline\n",
    "\n",
    "Entra√Ænement et √©valuation de 5 algorithmes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# D√âFINITION DES MOD√àLES\n",
    "# ============================================================================\n",
    "\n",
    "models_m1 = {\n",
    "    'Ridge': Ridge(random_state=RANDOM_STATE),\n",
    "    'Lasso': Lasso(random_state=RANDOM_STATE, max_iter=2000),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'SVR': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models_m1['XGBoost'] = XGBRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\n",
    "\n",
    "print(f\"‚úì {len(models_m1)} mod√®les d√©finis pour Mod√®le 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ac943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENTRA√éNEMENT BASELINE - MOD√àLE 1\n",
    "# ============================================================================\n",
    "\n",
    "# Utiliser la fonction du module\n",
    "results_df_m1, trained_models_m1 = train_multiple_models(\n",
    "    models_m1,\n",
    "    X_train_m1_scaled,\n",
    "    y_train,\n",
    "    X_test_m1_scaled,\n",
    "    y_test,\n",
    "    cv=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Afficher r√©sultats\n",
    "print(\"\\nüìä R√©sultats Baseline - Mod√®le 1 :\\n\")\n",
    "display(results_df_m1[['model', 'test_r2', 'test_rmse_log', 'test_mae_log', \n",
    "                       'test_rmse_original', 'test_mape', 'overfitting_r2']].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ecad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALISATION COMPARAISON - MOD√àLE 1\n",
    "# ============================================================================\n",
    "\n",
    "plot_model_comparison(\n",
    "    results_df_m1,\n",
    "    title=\"Comparaison Baseline - Mod√®le 1 (Sans ENERGY STAR)\",\n",
    "    save_path=FIGURES_DIR / 'baseline_m1_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e180110",
   "metadata": {},
   "source": [
    "# Section 5 : MOD√àLE 1 - Optimisation\n",
    "\n",
    "Optimisation du meilleur mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIMISATION - MOD√àLE 1\n",
    "# ============================================================================\n",
    "\n",
    "# Identifier meilleur mod√®le\n",
    "best_model_name_m1 = results_df_m1.iloc[0]['model']\n",
    "best_model_baseline_m1 = trained_models_m1[best_model_name_m1]\n",
    "\n",
    "print(f\"üèÜ Meilleur mod√®le baseline : {best_model_name_m1}\")\n",
    "print(f\"   R¬≤ Test : {results_df_m1.iloc[0]['test_r2']:.4f}\")\n",
    "\n",
    "# Optimisation\n",
    "best_model_m1, best_params_m1, opt_time_m1 = optimize_model(\n",
    "    best_model_baseline_m1,\n",
    "    X_train_m1_scaled,\n",
    "    y_train,\n",
    "    n_iter=200,\n",
    "    cv=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Optimisation termin√©e\")\n",
    "print(f\"\\nMeilleurs param√®tres :\")\n",
    "for param, value in best_params_m1.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73609598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer mod√®le optimis√©\n",
    "optimized_metrics_m1 = evaluate_model(\n",
    "    best_model_m1,\n",
    "    X_train_m1_scaled, y_train,\n",
    "    X_test_m1_scaled, y_test,\n",
    "    model_name=f\"{best_model_name_m1} (optimis√©)\"\n",
    ")\n",
    "\n",
    "# Afficher r√©sum√©\n",
    "print(\"\\nüìä R√âSULTATS MOD√àLE 1 OPTIMIS√â :\")\n",
    "summary_m1 = calculate_metrics_summary(optimized_metrics_m1)\n",
    "display(summary_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c141ed",
   "metadata": {},
   "source": [
    "# Section 6 : MOD√àLE 1 - Analyse\n",
    "\n",
    "Feature importance et analyse des r√©sidus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE IMPORTANCE - MOD√àLE 1\n",
    "# ============================================================================\n",
    "\n",
    "importance_df_m1 = plot_feature_importance(\n",
    "    best_model_m1,\n",
    "    X_train_m1_scaled.columns,\n",
    "    top_n=20,\n",
    "    title=f\"Top 20 Features - Mod√®le 1 ({best_model_name_m1})\",\n",
    "    save_path=FIGURES_DIR / 'feature_importance_m1.png'\n",
    ")\n",
    "\n",
    "print(\"\\nTop 10 Features :\")\n",
    "display(importance_df_m1.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa968191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSE R√âSIDUS - MOD√àLE 1\n",
    "# ============================================================================\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_m1 = best_model_m1.predict(X_test_m1_scaled)\n",
    "\n",
    "# Visualisations\n",
    "plot_predictions(\n",
    "    y_test, y_pred_m1,\n",
    "    title=f\"Pr√©dictions - Mod√®le 1 ({best_model_name_m1})\",\n",
    "    save_path=FIGURES_DIR / 'predictions_m1.png'\n",
    ")\n",
    "\n",
    "# Distribution r√©sidus\n",
    "residuals_m1 = y_test - y_pred_m1\n",
    "plot_residuals_distribution(\n",
    "    residuals_m1,\n",
    "    title=\"Distribution R√©sidus - Mod√®le 1\",\n",
    "    save_path=FIGURES_DIR / 'residuals_m1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41a40e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì **SECTIONS SUIVANTES**\n",
    "\n",
    "## √Ä compl√©ter :\n",
    "\n",
    "### **Section 7-9 : MOD√àLE 2 (Avec ENERGY STAR)**\n",
    "- M√™me structure que Sections 4-6\n",
    "- Changer `_m1` ‚Üí `_m2`\n",
    "\n",
    "### **Section 10 : Comparaison des 2 Mod√®les**\n",
    "```python\n",
    "# Utiliser la fonction de comparaison\n",
    "comparison = compare_models(results_df_m1, results_df_m2, \"Mod√®le 1\", \"Mod√®le 2\")\n",
    "display(comparison)\n",
    "\n",
    "# Visualisation\n",
    "plot_comparison_two_models(results_df_m1, results_df_m2, \n",
    "                           \"Sans ENERGY STAR\", \"Avec ENERGY STAR\")\n",
    "```\n",
    "\n",
    "### **Section 11 : Sauvegarde**\n",
    "```python\n",
    "# Sauvegarder mod√®les\n",
    "save_model(best_model_m1, MODELS_DIR / 'model1_best.pkl', optimized_metrics_m1)\n",
    "save_model(best_model_m2, MODELS_DIR / 'model2_best.pkl', optimized_metrics_m2)\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
